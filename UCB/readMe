What is the Multi-Armed Bandit Problem (MABP)?

A bandit is defined as someone who steals your money. A one-armed bandit is a simple slot machine wherein you insert a coin 
into the machine, pull a lever, and get an immediate reward.

But why is it called a bandit? 


It turns out all casinos configure these slot machines in such a way that all gamblers end up losing money!
A multi-armed bandit is a complicated slot machine wherein instead of 1, 
there are several levers which a gambler can pull, with each lever giving a different return. 
The probability distribution for the reward corresponding to each lever is different and is unknown to the gambler.

The task is to identify which lever to pull in order to get maximum reward after a given set of trials. This problem statement is like a single step Markov decision process, which I discussed in this article. Each arm chosen is equivalent to an action, which then leads to an immediate reward.


Upper Confidence Bound :

Upper Confidence Bound (UCB) is the most widely used solution method for multi-armed bandit problems. This algorithm is based on the principle of optimism in the face of uncertainty.
